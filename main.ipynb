{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxXMwqR-zbMx",
        "outputId": "8f302788-6cc5-4b1e-c054-5b69a5848687"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "import json\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WtiN2nxrzlLw"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "    od.download(\"https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\")\n",
        "    csv_path = \"ecommerce-text-classification/ecommerceDataset.csv\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.iloc[:, :2]\n",
        "    df.columns = [\"category\", \"description\"]\n",
        "    df.dropna(inplace=True)\n",
        "    df[\"description\"] = df[\"description\"].astype(str)\n",
        "    df[\"category\"] = df[\"category\"].astype(str)\n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess_text(df):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)\n",
        "        words = text.split()\n",
        "        words = [w for w in words if w not in stop_words]\n",
        "        words = [lemmatizer.lemmatize(w) for w in words]\n",
        "        text = ' '.join(words)\n",
        "        return text\n",
        "\n",
        "    df['clean_description'] = df['description'].apply(clean_text)\n",
        "    return df\n",
        "import numpy as np\n",
        "\n",
        "def compute_tfidf(corpus):\n",
        "    docs = [doc.split() for doc in corpus]\n",
        "    vocab = sorted(list(set([w for doc in docs for w in doc])))\n",
        "    N = len(docs)\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    tf = np.zeros((N, len(vocab)))\n",
        "    df = np.zeros(len(vocab))\n",
        "\n",
        "    for doc_idx, doc in enumerate(docs):\n",
        "        word_counts = {}\n",
        "        for word in doc:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "        for word, count in word_counts.items():\n",
        "            idx = vocab_index[word]\n",
        "            tf[doc_idx, idx] = count / len(doc)\n",
        "            df[idx] += 1\n",
        "\n",
        "    idf = np.log(N / (df + 1))\n",
        "    tfidf = tf * idf\n",
        "\n",
        "    return tfidf, vocab\n",
        "\n",
        "def train_word2vec(corpus, vector_size=75, window=5, min_count=1, workers=4):\n",
        "  sentences = [doc.split() for doc in corpus]\n",
        "  model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iCIxdPszoUq",
        "outputId": "e25bb692-6381-4d80-fac7-c2ad568ec3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: alirezasaffar\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\n",
            "Downloading ecommerce-text-classification.zip to ./ecommerce-text-classification\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7.86M/7.86M [00:00<00:00, 1.02GB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "df = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QIx43kSW1_Zx"
      },
      "outputs": [],
      "source": [
        "new_df = preprocess_text(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E04CtgNK3pvh"
      },
      "outputs": [],
      "source": [
        "new_df.to_csv(\"cleaned_dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OhmwZ8ll6SCd"
      },
      "outputs": [],
      "source": [
        "tfidf, vocab=compute_tfidf(new_df)\n",
        "model=train_word2vec(new_df)\n",
        "w2v_model = train_word2vec(new_df['clean_description'], vector_size=75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MuUjj0-u9DVU"
      },
      "outputs": [],
      "source": [
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "\n",
        "np.save('tfidf.npy', tfidf)\n",
        "with open('vocab.json', 'w') as f:\n",
        "    json.dump(vocab, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#how to restore : \n",
        "#from gensim.models import Word2Vec\n",
        "# w2v_model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# tfidf_loaded = np.load('tfidf.npy')\n",
        "# with open('vocab.json', 'r') as f:\n",
        "#     vocab_loaded = json.load(f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
